{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_1, meta_1 = pyreadstat.read_sav( '../../_data/endes/2019/REC0111.sav' )\n",
    "\n",
    "var_labels1 = meta_1.column_names_to_labels\n",
    "\n",
    "value_labels1 = meta_1.variable_value_labels\n",
    "\n",
    "rec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_2, meta_2 = pyreadstat.read_sav( '../../_data/endes/2019/RE223132.sav' )\n",
    "\n",
    "var_labels2 = meta_2.column_names_to_labels\n",
    "\n",
    "value_labels2 = meta_2.variable_value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_3, meta_3 = pyreadstat.read_sav( '../../_data/endes/2019/RE516171.sav' )\n",
    "\n",
    "var_labels3 = meta_3.column_names_to_labels\n",
    "\n",
    "value_labels3 = meta_3.variable_value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1 = rec_1.loc[ : ,['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V007', 'V008', 'V009', 'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V127', 'V133']]\n",
    "rec2_1 = rec_2.loc[ : ,['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']]\n",
    "rec3_1 = rec_3.loc[ : ,['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']]\n",
    "#Se selecciona las columnas para el siguiente dataframe \n",
    "#Se repite el procedimiento con las demás dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns_1 = rec1_1.columns.values \n",
    "#estamos interesados en quedarnos con las etiquetas de variables y de valores que apliquen para las variables de la base acotada \n",
    "new_value_labels1 = { column:value for ( column, value ) in value_labels1.items() if column in new_columns_1 } #Por ello generamos nuevos diccionarios para las etiquetas que cumplan la condición de que sus llaves forman parte del array de strings: new_keys1, new_keys2 y new_keys3 (según corresponda)\n",
    "new_var_labels1 = { column:value for ( column, value ) in var_labels1.items() if column in new_columns_1 }\n",
    "\n",
    "#replicamos el código previo para la base acotada de RE223132\n",
    "new_columns_2 = rec2_1.columns.values\n",
    "new_value_labels2 = { column:value for ( column, value ) in value_labels2.items() if column in new_columns_2 }\n",
    "new_var_labels2 = { column:value for ( column, value ) in var_labels2.items() if column in new_columns_2 }\n",
    "\n",
    "#replicamos el código previo para la base acotada de RE516171 \n",
    "new_columns_3 = rec3_1.columns.values\n",
    "new_value_labels3 = { column:value for ( column, value ) in value_labels3.items() if column in new_columns_3 }\n",
    "new_var_labels3 = { column:value for ( column, value ) in var_labels3.items() if column in new_columns_3 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos loc para asignar el valor 2019 en la columna nueva year en el df rec1_1:\n",
    "rec1_1.loc[:,\"year\"] = 2019\n",
    "\n",
    "rec1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para actualizar las etiquetas de los datos utilizamos .update:\n",
    "new_var_labels1.update({\"year\": \"Year of the survey\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza el merge de las primeras dos variables \"rec_1_1\" y \"rec_2_1\" utilizando como relación la columna \"CASEID\"\n",
    "endes_2019 = pd.merge(rec1_1,rec2_1, on='CASEID')\n",
    "\n",
    "# Se realiza el merge de la unión de las dos bases anteriores con la base \"rec_3_1\" utilizando como relación la columna \"CASEID\"\n",
    "endes_2019 = pd.merge(endes_2019,rec3_1, on='CASEID')\n",
    "# Se muestra el resultado final, en el cual se observan 64 columnas correspondientes a la suma de las columnas de las 3 bases de datos menos dos columnas \"CASEID\"\n",
    "endes_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea una copia a la variable \"new_var_labels1\" con la denominación \"var_labels\".\n",
    "var_labels = new_var_labels1\n",
    "#Se actualizan las variables incluyendo las que se encuentran en \"new_var_labels2\".\n",
    "var_labels.update(new_var_labels2)\n",
    "#Se actualiza otra vez incluyendo las variables de \"new_var_labels3\"\n",
    "var_labels.update(new_var_labels3)\n",
    "# Verificamos la cantidad de variables, las cuales deben coincidir con la cantidad de columnas que se encuentran en la df_2019.\n",
    "var_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir del código de chat gpt intento Nº2 \n",
    "# Definir la lista de años y archivos\n",
    "years = ['2015', '2016', '2017', '2018', '2019']\n",
    "files = ['REC0111.sav', 'RE223132.sav', 'RE516171.sav']\n",
    "\n",
    "# Directorio base donde se encuentran las carpetas por año\n",
    "base_directory = '../../_data/endes/'\n",
    "\n",
    "# Iterar sobre los años y archivos\n",
    "for year in years:\n",
    "    for file in files:\n",
    "        # Crear la ruta completa al archivo\n",
    "        file_path = os.path.join(base_directory, year, file)\n",
    "        \n",
    "        try:\n",
    "            # Intentar leer el archivo\n",
    "            rec, meta = pyreadstat.read_sav(file_path)\n",
    "            \n",
    "            # Realizar operaciones con los datos si es necesario\n",
    "            # ...\n",
    "\n",
    "            # Imprimir información o realizar otras acciones\n",
    "            print(f\"Archivo leído exitosamente: {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Manejar cualquier excepción que pueda ocurrir al intentar leer el archivo\n",
    "            print(f\"Error al leer el archivo {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of years\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "\n",
    "# Define the base directory\n",
    "base_directory = f'../../_data/endes/{year}/'\n",
    "\n",
    "# Initialize the nested dictionary\n",
    "all_data = {}\n",
    "\n",
    "# Iterate over the years\n",
    "for year in years:\n",
    "    \n",
    "    # Initialize the inner dictionary for each year\n",
    "    year_data = {'data': None, 'var_labels': None, 'value_labels': None}\n",
    "    \n",
    "    # Import data for each file so we follow the steps on the first items\n",
    "    rec_1, meta_1 = pyreadstat.read_sav(os.path.join(base_directory, 'REC0111.sav'))\n",
    "    rec_2, meta_2 = pyreadstat.read_sav(os.path.join(base_directory, 'RE223132.sav'))\n",
    "    rec_3, meta_3 = pyreadstat.read_sav(os.path.join(base_directory, 'RE516171.sav'))\n",
    "    \n",
    "    # Select necessary columns for each data set\n",
    "    rec1_1 = rec_1.loc[:, ['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V008', 'V009', 'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V127', 'V133']]\n",
    "    rec2_1 = rec_2.loc[:, ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']]\n",
    "    rec3_1 = rec_3.loc[:, ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']]\n",
    "    \n",
    "    # Generate a new column for rec1_1 named as year\n",
    "    rec1_1.loc[:, 'year'] = year\n",
    "    \n",
    "    # Update var_labels dictionary\n",
    "    var_labels = {}\n",
    "    var_labels.update(meta_1.column_names_to_labels)\n",
    "    var_labels.update(meta_2.column_names_to_labels)\n",
    "    var_labels.update(meta_3.column_names_to_labels)\n",
    "    \n",
    "    # Update value_labels dictionary\n",
    "    value_labels = {}\n",
    "    value_labels.update(meta_1.variable_value_labels)\n",
    "    value_labels.update(meta_2.variable_value_labels)\n",
    "    value_labels.update(meta_3.variable_value_labels)\n",
    "    \n",
    "    # Merge the 3 datasets\n",
    "    endes_total = rec1_1.merge(rec2_1, on='CASEID')\n",
    "    endes_total = endes_total.merge(rec3_1, on='CASEID')\n",
    "    \n",
    "    # Store data, var_labels, and value_labels in the nested dictionary\n",
    "    all_data[f'year_{year}'] = {'data': endes_total, 'var_labels': var_labels, 'value_labels': value_labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['year_2017']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of years\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "\n",
    "# Define the list of files\n",
    "files = ['REC0111.sav', 'RE223132.sav', 'RE516171.sav']\n",
    "\n",
    "# Define the base directory\n",
    "base_directory = '../../_data/endes/'\n",
    "\n",
    "# Initialize an empty list to store individual DataFrames\n",
    "all_data_list = []\n",
    "\n",
    "# Iterate over the years\n",
    "for year in years:\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(base_directory, year, file)\n",
    "        \n",
    "        try:\n",
    "            # Read the file\n",
    "            data, meta = pyreadstat.read_sav(file_path)\n",
    "            data['year'] = year\n",
    "            # Append the DataFrame to the list\n",
    "            all_data_list.append(data)\n",
    "            \n",
    "            # Print information or perform other actions\n",
    "            print(f\"File read successfully: {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that may occur during file reading\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "endes_data_2015_2019 = pd.concat(all_data_list, ignore_index=True)\n",
    "\n",
    "# Print the concatenated DataFrame\n",
    "print(endes_data_2015_2019.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the data\n",
    "new_data = []\n",
    "\n",
    "# Iterate for each year in all_data using key value for each year in data so we can append it\n",
    "for year_key, year_data in all_data.items():\n",
    "    new_data.append(year_data['data'])\n",
    "\n",
    "# Concatenate all data sets in the list using pd.concat\n",
    "endes_data_2015_2019 = pd.concat(new_data, ignore_index=True)\n",
    "\n",
    "print(endes_data_2015_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(endes_data_2015_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded the data and stored it in 'endes_data_2015_2019'\n",
    "\n",
    "import os\n",
    "import pyreadstat\n",
    "\n",
    "# Define the list of years and files\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "files = ['REC0111.sav', 'RE223132.sav', 'RE516171.sav']\n",
    "\n",
    "# Define the base directory\n",
    "base_directory = '../../_data/endes/'\n",
    "\n",
    "# Initialize dictionaries for var_labels and value_labels\n",
    "all_var_labels = {}\n",
    "all_value_labels = {}\n",
    "\n",
    "# Iterate over the years\n",
    "for year in years:\n",
    "    var_labels = {}\n",
    "    value_labels = {}\n",
    "    \n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base_directory, year, file)\n",
    "        \n",
    "        try:\n",
    "            # Read the file\n",
    "            data, meta = pyreadstat.read_sav(file_path)\n",
    "            \n",
    "            # Store var_labels and value_labels in dictionaries\n",
    "            var_labels.update(meta.column_names_to_labels)\n",
    "            value_labels.update(meta.variable_value_labels)\n",
    "            \n",
    "            print(f\"File read successfully: {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Store var_labels and value_labels for each year in the main dictionaries\n",
    "    all_var_labels[year] = var_labels\n",
    "    all_value_labels[year] = value_labels\n",
    "\n",
    "# Store var_labels and value_labels as attributes in endes_data_2015_2019\n",
    "endes_data_2015_2019.var_labels = all_var_labels\n",
    "endes_data_2015_2019.value_labels = all_value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(all_var_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables\n",
    "new_data_2015_2019 = endes_data_2015_2019[[\"V201\", \"V613\", \"V715\", \"V511\", \"year\", \"V024\"]]\n",
    "# Convert variables to numerical\n",
    "cols = [\"V201\", \"V613\", \"V715\", \"V511\"]\n",
    "new_data_2015_2019[cols] = new_data_2015_2019[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate averages by year and department using mean command\n",
    "mean_key_vars = new_data_2015_2019.groupby([\"year\",'V024']).mean()\n",
    "mean_key_vars\n",
    "# Rename columns as needed\n",
    "mean_key_vars.columns = ['mean_total_children', 'mean_ideal_children', 'mean_hb_yr_educ', 'mean_first_marriage']\n",
    "mean_key_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset indexes as columns (year and department)\n",
    "mean_key_vars = mean_key_vars.reset_index()\n",
    "#Merge on year and department to the main base\n",
    "final_result = pd.merge(endes_data_2015_2019,mean_key_vars , on=['year','V024'])\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
